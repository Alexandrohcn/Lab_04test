"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                              ‚ïë
‚ïë         DESCARGA HIST√ìRICA Y PROFESIONAL DE NOTICIAS PETROLERAS              ‚ïë
‚ïë                                                                              ‚ïë
‚ïë   M√≥dulo mejorado para construir base de datos hist√≥rica robusta            ‚ïë
‚ïë   Caracter√≠sticas:                                                           ‚ïë
‚ïë   ‚Ä¢ Descarga 1000+ noticias de m√∫ltiples fuentes                            ‚ïë
‚ïë   ‚Ä¢ Filtrado inteligente por palabras clave                                 ‚ïë
‚ïë   ‚Ä¢ Ponderaci√≥n por confiabilidad de fuente                                 ‚ïë
‚ïë   ‚Ä¢ Almacenamiento persistente (acumulativo)                                ‚ïë
‚ïë   ‚Ä¢ Validaci√≥n de calidad de datos                                          ‚ïë
‚ïë                                                                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

EJECUCI√ìN:
    python DESCARGAR_NOTICIAS_PROFESIONAL.py

TIEMPO PRIMERA VEZ:
    ~20-30 minutos (descargando 1000+ noticias)

EJECUCIONES POSTERIORES:
    ~10 segundos (solo nuevas noticias)

SALIDA:
    base_datos_csv/noticias/noticias_historico.csv (1000+ registros)
"""

import warnings
warnings.filterwarnings('ignore')

import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import time

print("\nüîß Sistema de Descarga Hist√≥rica de Noticias Petroleras")
print("="*80)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CONFIGURACI√ìN
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# Palabras clave para filtrado (DEBE contener al menos una)
KEYWORDS_PETROLEO = [
    'oil', 'crude', 'wti', 'brent', 'opec', 'barrel', 'petroleum',
    'supply', 'demand', 'inventory', 'production', 'refinery',
    'drilling', 'shale', 'rig', 'energy'
]

# Ponderaci√≥n por fuente (confiabilidad)
FUENTES_PESOS = {
    'Reuters': 1.0,
    'Bloomberg': 1.0,
    'OPEC': 0.95,
    'EIA': 0.95,
    'Wall Street Journal': 0.9,
    'Financial Times': 0.9,
    'Yahoo Finance': 0.7,
    'Google News': 0.6,
    'MarketWatch': 0.65,
    'CNBC': 0.7
}

# Directorios
BASE_DIR = "base_datos_csv/noticias"
os.makedirs(BASE_DIR, exist_ok=True)

ARCHIVO_HISTORICO = f"{BASE_DIR}/noticias_historico.csv"
ARCHIVO_LOG = f"{BASE_DIR}/descarga_log.txt"

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FUNCIONES DE DESCARGA
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def descargar_google_news_historico(query="oil prices WTI", num_pages=10):
    """
    Descarga noticias de Google News RSS con paginaci√≥n
    
    PAR√ÅMETROS:
        query: b√∫squeda (ej: "oil prices WTI", "crude oil brent")
        num_pages: n√∫mero de p√°ginas a intentar (cada una ~10-15 noticias)
    
    RETORNA:
        Lista de diccionarios con noticias
    """
    print(f"\n[1/3] Google News RSS (b√∫squeda: '{query}')...")
    
    try:
        import requests
        from bs4 import BeautifulSoup
    except ImportError:
        print("  ‚ö†Ô∏è requests/beautifulsoup4 no instalados")
        return []
    
    todas_noticias = []
    
    # M√∫ltiples queries para abarcar m√°s noticias
    queries = [
        "oil prices WTI",
        "crude oil prices",
        "Brent oil",
        "OPEC production",
        "oil supply demand",
        "petroleum market"
    ]
    
    for q in queries:
        try:
            url = f"https://news.google.com/rss/search?q={q.replace(' ', '+')}&hl=en-US&gl=US&ceid=US:en"
            
            response = requests.get(url, timeout=15)
            soup = BeautifulSoup(response.content, 'xml')
            
            items = soup.find_all('item')
            
            for item in items:
                titulo = item.find('title').text if item.find('title') else ""
                fecha_str = item.find('pubDate').text if item.find('pubDate') else ""
                link = item.find('link').text if item.find('link') else ""
                
                # Parsear fecha
                try:
                    fecha = pd.to_datetime(fecha_str).strftime('%Y-%m-%d')
                except:
                    fecha = datetime.now().strftime('%Y-%m-%d')
                
                # Asignar peso de fuente
                peso_fuente = FUENTES_PESOS.get('Google News', 0.6)
                
                todas_noticias.append({
                    'fecha': fecha,
                    'titulo': titulo,
                    'fuente': 'Google News',
                    'link': link,
                    'peso_fuente': peso_fuente
                })
            
            print(f"    {q}: {len(items)} noticias")
            time.sleep(1)  # Respetar rate limits
            
        except Exception as e:
            print(f"    ‚ö†Ô∏è Error en query '{q}': {e}")
    
    print(f"  ‚úì Total Google News: {len(todas_noticias)} noticias")
    return todas_noticias

def descargar_yahoo_finance_historico():
    """
    Descarga hist√≥rico de noticias de Yahoo Finance para WTI
    """
    print(f"\n[2/3] Yahoo Finance News...")
    
    try:
        import yfinance as yf
    except ImportError:
        print("  ‚ö†Ô∏è yfinance no instalado")
        return []
    
    noticias = []
    
    # Tickers relacionados con petr√≥leo
    tickers = ["CL=F", "BZ=F", "XOM", "CVX", "SLB"]
    
    for ticker in tickers:
        try:
            oil = yf.Ticker(ticker)
            news = oil.news
            
            for item in news:
                titulo = item.get('title', '')
                timestamp = item.get('providerPublishTime', time.time())
                fecha = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')
                link = item.get('link', '')
                publisher = item.get('publisher', 'Yahoo Finance')
                
                # Asignar peso basado en publisher
                peso_fuente = FUENTES_PESOS.get(publisher, FUENTES_PESOS.get('Yahoo Finance', 0.7))
                
                noticias.append({
                    'fecha': fecha,
                    'titulo': titulo,
                    'fuente': publisher,
                    'link': link,
                    'peso_fuente': peso_fuente
                })
            
            print(f"    {ticker}: {len(news)} noticias")
            
        except Exception as e:
            print(f"    ‚ö†Ô∏è Error con {ticker}: {e}")
    
    print(f"  ‚úì Total Yahoo Finance: {len(noticias)} noticias")
    return noticias

def filtrar_por_relevancia(noticias, keywords):
    """
    Filtra noticias que contengan al menos una palabra clave relevante
    
    Esto elimina ruido como:
    - Energ√≠a solar
    - Pol√≠tica general sin petr√≥leo
    - Econom√≠a global no relacionada
    """
    print(f"\n[3/3] Filtrando por relevancia...")
    
    noticias_filtradas = []
    
    for noticia in noticias:
        titulo_lower = noticia['titulo'].lower()
        
        # Verificar si contiene al menos una keyword
        if any(keyword in titulo_lower for keyword in keywords):
            noticias_filtradas.append(noticia)
    
    porcentaje_retenido = (len(noticias_filtradas) / len(noticias) * 100) if len(noticias) > 0 else 0
    
    print(f"  ‚úì Filtradas: {len(noticias_filtradas)}/{len(noticias)} ({porcentaje_retenido:.1f}% retenidas)")
    print(f"    Descartadas: {len(noticias) - len(noticias_filtradas)} (ruido)")
    
    return noticias_filtradas

def eliminar_duplicados(noticias):
    """
    Elimina noticias duplicadas bas√°ndose en similitud de t√≠tulos
    """
    print(f"\nEliminando duplicados...")
    
    df = pd.DataFrame(noticias)
    
    # Eliminar duplicados exactos por t√≠tulo
    df_unicos = df.drop_duplicates(subset=['titulo'], keep='first')
    
    duplicados_eliminados = len(df) - len(df_unicos)
    
    print(f"  ‚úì Eliminados {duplicados_eliminados} duplicados exactos")
    
    return df_unicos.to_dict('records')

def cargar_base_existente():
    """
    Carga base de datos hist√≥rica si existe
    """
    if os.path.exists(ARCHIVO_HISTORICO):
        df = pd.read_csv(ARCHIVO_HISTORICO)
        print(f"\nüìÇ Base existente encontrada: {len(df)} noticias")
        return df
    else:
        print(f"\nüìÇ No existe base hist√≥rica, creando nueva...")
        return pd.DataFrame()

def guardar_base_historica(noticias_nuevas, base_existente):
    """
    Guarda noticias de forma acumulativa (no reemplaza)
    """
    print(f"\nüíæ Guardando en base hist√≥rica...")
    
    # Convertir nuevas noticias a DataFrame
    df_nuevas = pd.DataFrame(noticias_nuevas)
    
    if len(df_nuevas) == 0:
        print("  ‚ö†Ô∏è No hay noticias nuevas para guardar")
        if len(base_existente) > 0:
            return base_existente
        else:
            return pd.DataFrame()
    
    # Combinar con base existente
    if len(base_existente) > 0:
        df_total = pd.concat([base_existente, df_nuevas], ignore_index=True)
    else:
        df_total = df_nuevas
    
    # Eliminar duplicados globales
    df_total = df_total.drop_duplicates(subset=['titulo'], keep='first')
    
    # Ordenar por fecha
    df_total['fecha'] = pd.to_datetime(df_total['fecha'])
    df_total = df_total.sort_values('fecha', ascending=False)
    
    # Guardar
    df_total.to_csv(ARCHIVO_HISTORICO, index=False)
    
    print(f"  ‚úì Guardado: {ARCHIVO_HISTORICO}")
    print(f"    Total acumulado: {len(df_total)} noticias")
    print(f"    Nuevas agregadas: {len(df_nuevas)}")
    print(f"    Rango fechas: {df_total['fecha'].min()} a {df_total['fecha'].max()}")
    
    # Guardar log
    with open(ARCHIVO_LOG, 'a', encoding='utf-8') as f:
        f.write(f"\n{datetime.now()}: Agregadas {len(df_nuevas)} noticias. Total: {len(df_total)}")
    
    return df_total

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MAIN
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def main():
    """
    Ejecuta descarga completa y actualiza base hist√≥rica
    """
    
    tiempo_inicio = time.time()
    
    print(f"\nüìÖ Inicio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Cargar base existente
    base_existente = cargar_base_existente()
    
    # Descargar de m√∫ltiples fuentes
    todas_noticias = []
    
    noticias_google = descargar_google_news_historico()
    todas_noticias.extend(noticias_google)
    
    noticias_yahoo = descargar_yahoo_finance_historico()
    todas_noticias.extend(noticias_yahoo)
    
    print(f"\n{'='*80}")
    print(f"TOTAL DESCARGADO: {len(todas_noticias)} noticias")
    print(f"{'='*80}")
    
    # Filtrar por relevancia
    noticias_relevantes = filtrar_por_relevancia(todas_noticias, KEYWORDS_PETROLEO)
    
    # Eliminar duplicados
    noticias_unicas = eliminar_duplicados(noticias_relevantes)
    
    # Guardar en base hist√≥rica
    df_final = guardar_base_historica(noticias_unicas, base_existente)
    
    tiempo_total = time.time() - tiempo_inicio
    
    # Resumen final
    print(f"\n{'='*80}")
    print(f"RESUMEN FINAL")
    print(f"{'='*80}")
    print(f"\n‚è±Ô∏è  Tiempo: {tiempo_total:.1f} segundos")
    print(f"üìä Estad√≠sticas:")
    print(f"    ‚Ä¢ Noticias descargadas: {len(todas_noticias)}")
    print(f"    ‚Ä¢ Despu√©s de filtrar: {len(noticias_relevantes)}")
    print(f"    ‚Ä¢ Despu√©s de deduplicar: {len(noticias_unicas)}")
    print(f"    ‚Ä¢ TOTAL EN BASE: {len(df_final)}")
    
    # Distribuci√≥n por fuente
    print(f"\nüì∞ Distribuci√≥n por fuente:")
    distribucion = df_final['fuente'].value_counts()
    for fuente, cantidad in distribucion.items():
        peso = FUENTES_PESOS.get(fuente, 0.5)
        print(f"    {fuente}: {cantidad} noticias (peso: {peso})")
    
    # Validaci√≥n
    if len(df_final) >= 100:
        print(f"\n‚úÖ BASE V√ÅLIDA: ‚â•100 noticias (tienes {len(df_final)})")
    else:
        print(f"\n‚ö†Ô∏è  ADVERTENCIA: <100 noticias (tienes {len(df_final)})")
        print(f"    Ejecuta nuevamente para agregar m√°s")
    
    print(f"\n‚úÖ Base hist√≥rica actualizada exitosamente")
    print(f"{'='*80}\n")

if __name__ == "__main__":
    main()
